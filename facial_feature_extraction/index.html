<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Facial Feature Extraction</title>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        #canvas {
            border: 1px solid black;
        }
    </style>
</head>
<body>
    <h1>Facial Feature Extraction</h1>
    <input type="file" id="imageInput" accept="image/*">
    <button id="extractFeaturesButton" disabled>Extract Features</button>
    <canvas id="canvas" width="300" height="300"></canvas>
    <div id="featuresOutput"></div>

    <script src="https://docs.opencv.org/4.5.4/opencv.js" type="text/javascript"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js"></script> 
    <script>
        // Load Face Detection Model
        let model;
        async function loadModel() {
            model = await tf.loadLayersModel('https://storage.googleapis.com/tfjs-models/tfjs/face_detection/model.json');
        }
        loadModel();
        // Step 1: Get input elements
        const imageInput = document.getElementById('imageInput');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const extractFeaturesButton = document.getElementById('extractFeaturesButton');
        const featuresOutput = document.getElementById('featuresOutput');

        // Step 2: Handle image input change
        imageInput.addEventListener('change', (e) => {
            // Step 3: Read selected image
            const file = e.target.files[0];
            const reader = new FileReader();
            reader.onload = () => {
                // Step 4: Draw image on canvas
                const img = new Image();
                img.onload = () => {
                    drawImage(img);
                    // Step 5: Enable extract features button
                    extractFeaturesButton.disabled = false;
                };
                img.src = reader.result;
            };
            reader.readAsDataURL(file);
        });

        function drawImage(img){
            canvas.width = img.width;
            canvas.height = img.height;
            ctx.drawImage(img, 0, 0);
        }

        // Step 6: Handle extract features button click
        extractFeaturesButton.addEventListener('click', async () => {
            // Step 7: Load OpenCV 
            console.log(window.cv.load)
            const cv = window.cv;
            // Step 9: Convert canvas to OpenCV matrix 
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
            const img = cv.matFromImageData(imageData);

            console.log(9);
            cv.imshow('canvas', img);
            // Step 10: Convert matrix to image  
            const gray = new cv.Mat();
            cv.cvtColor(img, gray, cv.COLOR_RGBA2GRAY, 0); 

            console.log(10);
            cv.imshow('canvas', gray); 
            //
            // Step 11: Extract face region
            const extractFaceRegion = async (img) => {
                // Preprocess image
                const imgTensor = tf.browser.fromPixels(img);
                const resizedImg = tf.image.resizeBilinear(imgTensor, [160, 160]);
                const normalizedImg = resizedImg.toFloat().div(255);

                // Extract face region
                const faceRegion = normalizedImg.slice([0, 0, 0], [160, 160, 3]);

                return faceRegion;
            };

            const faceRegion = await extractFaceRegion(gray);
            console.log(11);
            // Step 12: Resize face image
            const resizeFaceImage = async (faceRegion) => {
                // Resize face image
                const resizedFaceImage = tf.image.resizeBilinear(faceRegion, [112, 112]);

                return resizedFaceImage;
            };

            const resizedFaceImage = await resizeFaceImage(faceRegion);
            console.log(12);
            // Step 13: Load MobileFaceNet model
            const mobileFaceNetModel = await tf.loadLayersModel('https://storage.googleapis.com/tfjs-models/tfjs/mobilenet/model.json');

            console.log(13);
            // Step 14: Extract facial features
            const extractFacialFeatures = async (resizedFaceImage) => {
                // Extract facial features
                const features = mobileFaceNetModel.predict(resizedFaceImage.expandDims(0));

                return features;
            };

            const features = await extractFacialFeatures(resizedFaceImage);
            console.log(14);
            // Step 15: Display features
            const displayFeatures = async (features) => {
                // Display features
                console.log(features);
            };
 
            await displayFeatures(features);
            console.log(15);
            
            // Step 17: Release resources
            img.delete();
            gray.delete();
            faceImg.delete();
            alignedFace.delete();
            tensor.dispose();
            inputs.dispose();
            features.dispose();

            console.log(17);
            
        });
    </script>
</body>
</html>
